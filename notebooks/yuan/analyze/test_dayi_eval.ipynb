{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=\"sk-WeX73uM3ERd6f7kF6b373aFbE62544B1A06fEf93C67156B6\",\n",
    "                base_url=\"https://api.emabc.xyz/v1\")\n",
    "\n",
    "TEMPLATE = \"\"\"请总结该医疗报告中的诊断关键词，以分号分隔：\n",
    "\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_NA = \"No context available.\"\n",
    "\n",
    "\n",
    "def add_message_to_history(dialogue_history, role, content):\n",
    "    \"\"\"将消息添加到对话历史中。\"\"\"\n",
    "    dialogue_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "\n",
    "def generate_response(prompt):\n",
    "    \"\"\"使用 OpenAI GPT-4 生成回复。\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=prompt, temperature=1.0, stop=\"\"  # 确保使用正确的引擎名\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message.content\n",
    "    # function_call = response.choices[0].message.tool_calls\n",
    "    return message\n",
    "\n",
    "\n",
    "def create_prompt_from_history(dialogue_history, instruction=\"\"):\n",
    "    \"\"\"从对话历史中创建提示文本。\"\"\"\n",
    "\n",
    "    prompt = [{\"role\": \"system\", \"content\": instruction}]\n",
    "    for entry in dialogue_history:\n",
    "        prompt.append({\"role\": entry[\"role\"], \"content\": entry[\"content\"]})\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_response(dialogue_history, instruction=\"\"):\n",
    "    prompt = create_prompt_from_history(dialogue_history, instruction)\n",
    "    message = generate_response(prompt)\n",
    "    return message\n",
    "\n",
    "\n",
    "def get_client():\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help. (request id: 2025022516180494572068028428584)', 'type': 'invalid_request_error', 'param': '', 'code': 'account_deactivated'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m instruction \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful and precise assistant for checking the quality of the answer of medical VQA task\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m dialogue_history \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: query,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m response \u001b[39m=\u001b[39m get_response(dialogue_history, instruction)\n",
      "\u001b[1;32m/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(dialogue_history, instruction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     prompt \u001b[39m=\u001b[39m create_prompt_from_history(dialogue_history, instruction)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     message \u001b[39m=\u001b[39m generate_response(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m message\n",
      "\u001b[1;32m/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(prompt):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"使用 OpenAI GPT-4 生成回复。\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4o\u001b[39m\u001b[39m\"\u001b[39m, messages\u001b[39m=\u001b[39mprompt, temperature\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, stop\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# 确保使用正确的引擎名\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     message \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yesindeed/Documents/Research/codes/ours/MedVLMBench/notebooks/yuan/analyze/test_gpt_eval.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# function_call = response.choices[0].message.tool_calls\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlmbenchmark/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vlmbenchmark/lib/python3.11/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    817\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m    818\u001b[0m             {\n\u001b[1;32m    819\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[1;32m    820\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m    821\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m: audio,\n\u001b[1;32m    822\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    823\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[1;32m    824\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[1;32m    825\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[1;32m    826\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m: logprobs,\n\u001b[1;32m    827\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_completion_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    828\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[1;32m    829\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: metadata,\n\u001b[1;32m    830\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodalities\u001b[39m\u001b[39m\"\u001b[39m: modalities,\n\u001b[1;32m    831\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m    832\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mparallel_tool_calls\u001b[39m\u001b[39m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    833\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    834\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[1;32m    835\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[1;32m    836\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mservice_tier\u001b[39m\u001b[39m\"\u001b[39m: service_tier,\n\u001b[1;32m    837\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m    838\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstore\u001b[39m\u001b[39m\"\u001b[39m: store,\n\u001b[1;32m    839\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[1;32m    840\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream_options\u001b[39m\u001b[39m\"\u001b[39m: stream_options,\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[1;32m    843\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[1;32m    844\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_logprobs\u001b[39m\u001b[39m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    845\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m    846\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[1;32m    847\u001b[0m             },\n\u001b[1;32m    848\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    849\u001b[0m         ),\n\u001b[1;32m    850\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m    851\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    852\u001b[0m         ),\n\u001b[1;32m    853\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[1;32m    854\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m         stream_cls\u001b[39m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/vlmbenchmark/lib/python3.11/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/vlmbenchmark/lib/python3.11/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vlmbenchmark/lib/python3.11/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help. (request id: 2025022516180494572068028428584)', 'type': 'invalid_request_error', 'param': '', 'code': 'account_deactivated'}}"
     ]
    }
   ],
   "source": [
    "qs = \"what are positively charged, thus allowing the compaction of the negatively charged dna?\"\n",
    "ans = \"the histone subunits\"\n",
    "pred = \"the protein components of the replication fork\"\n",
    "\n",
    "prompt = \"We would like to request your feedback on the performance of the AI assistant in response to the user question displayed above, with reference to the provided ground truth answer. Please rate the helpfulness, relevance, accuracy, and level of detail of the assistant's response. Assign an overall score on a scale of 1 to 100, where a higher score indicates better overall performance. Please first output a single line containing only the score (a single numeric value). In the subsequent line, please provide a comprehensive explanation of your evaluation, referencing the ground truth answer to justify your score. Ensure your judgment is unbiased and objective.\"\n",
    "\n",
    "query = (\n",
    "    f\"[Question]\\n{qs}\\n\\n\"\n",
    "    f\"[True Answer]\\n{ans}\\n\\n[End of True Answer]\\n\\n\"\n",
    "    f\"[Prediction]\\n{pred}\\n\\n[End of prediction]\\n\\n\"\n",
    "    f\"[System]\\n{prompt}\\n\\n\"\n",
    ")\n",
    "\n",
    "instruction = \"You are a helpful and precise assistant for checking the quality of the answer of medical VQA task\"\n",
    "\n",
    "dialogue_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": query,\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_response(dialogue_history, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Question]\\nwhat are positively charged, thus allowing the compaction of the negatively charged dna?\\n\\n[True Answer]\\nthe histone subunits\\n\\n[End of True Answer]\\n\\n[Prediction]\\nthe protein components of the replication fork\\n\\n[End of prediction]\\n\\n[System]\\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above, with reference to the provided ground truth answer. Please rate the helpfulness, relevance, accuracy, and level of detail of the assistant's response. Assign an overall score on a scale of 1 to 100, where a higher score indicates better overall performance. Please first output a single line containing only the score (a single numeric value). In the subsequent line, please provide a comprehensive explanation of your evaluation, referencing the ground truth answer to justify your score. Ensure your judgment is unbiased and objective.\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmbenchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
